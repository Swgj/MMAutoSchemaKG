import streamlit as st
import os
import re
from dotenv import load_dotenv
from openai import OpenAI

# å¼•å…¥é¡¹ç›®ç»„ä»¶
from atlas_rag.llm_generator import LLMGenerator
from atlas_rag.vectorstore.embedding_model import EmbeddingAPI
from atlas_rag.multimodal.hipporag_adapter import Neo4jToHippoAdapter
from atlas_rag.retriever.hipporag import HippoRAGRetriever
from atlas_rag.retriever.inference_config import InferenceConfig
from atlas_rag.multimodal.multimodal_react import MultimodalReAct

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="Multimodal KG RAG Demo", layout="wide")
st.title("ğŸ§© Multimodal Knowledge Graph RAG")
st.markdown("Ask questions about your data, and see how the system retrieves both **Text** and **Images** from the Graph.")

# --- ä¾§è¾¹æ é…ç½® ---
with st.sidebar:
    st.header("Configuration")
    neo4j_uri = st.text_input("Neo4j URI", value="bolt://localhost:7687")
    neo4j_user = st.text_input("Neo4j User", value="neo4j")
    neo4j_password = st.text_input("Neo4j Password", value="password", type="password")
    
    st.divider()
    
    top_k = st.slider("Top-K Retrieval", min_value=1, max_value=10, value=3)
    hipporag_mode = st.selectbox("HippoRAG Mode", ["query2node", "query2edge", "ner"])

# --- æ ¸å¿ƒèµ„æºåŠ è½½ (å¸¦ç¼“å­˜) ---
@st.cache_resource
def load_system_resources(uri, user, pwd):
    """
    åˆå§‹åŒ–æ¨¡å‹å’ŒåŠ è½½å›¾è°±æ•°æ®ã€‚
    ä½¿ç”¨ cache_resource è£…é¥°å™¨ï¼Œç¡®ä¿åªåŠ è½½ä¸€æ¬¡ï¼Œä¸ç”¨æ¯æ¬¡åˆ·æ–°é¡µé¢éƒ½é‡è·‘ã€‚
    """
    api_key = os.getenv("GEMINI_API_KEY")
    base_url = os.getenv("GEMINI_BASE_URL")
    
    if not api_key:
        st.error("âŒ GEMINI_API_KEY not found! Please check your .env file.")
        return None, None, None

    with st.spinner("ğŸš€ Loading Knowledge Graph & Models... (This may take a moment)"):
        # 1. åˆå§‹åŒ–æ¨¡å‹
        client = OpenAI(api_key=api_key, base_url=base_url)
        # æ³¨æ„ï¼šè¿™é‡Œçš„æ¨¡å‹åè¦å’Œä½  vector_store.py é‡Œç®—å‘é‡æ—¶ç”¨çš„ä¸€è‡´
        embedding_model = EmbeddingAPI(client, model_name="gemini-embedding-001")
        llm_generator = LLMGenerator(client, model_name="gemini-2.5-flash")
        
        # 2. ä» Neo4j åŠ è½½æ•°æ®åˆ°å†…å­˜
        adapter = Neo4jToHippoAdapter(uri, user, pwd, embedding_model, database_name="locomo-hard-0")
        data = adapter.load_data()
        adapter.close()
        
        return llm_generator, embedding_model, data

# åŠ è½½èµ„æº
llm_gen, emb_model, kg_data = load_system_resources(neo4j_uri, neo4j_user, neo4j_password)

if not kg_data:
    st.stop() # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåœæ­¢è¿è¡Œ

# åˆå§‹åŒ– Retriever (é…ç½®å¯èƒ½éšä¾§è¾¹æ å˜åŒ–ï¼Œæ‰€ä»¥ä¸ç¼“å­˜è¿™ä¸ªå¯¹è±¡ï¼Œåªç¼“å­˜ data)
config = InferenceConfig()
config.hipporag_mode = hipporag_mode
config.topk_nodes = 20
config.ppr_alpha = 0.85

retriever = HippoRAGRetriever(
    llm_generator=llm_gen,
    sentence_encoder=emb_model,
    data=kg_data,
    inference_config=config
)

image_map = kg_data.get("image_map", {})

# --- ä¸»äº¤äº’åŒº ---

query = st.chat_input("Ask a question about Caroline, Melanie, or anything in the graph...")

if query:
    # 1. æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
    with st.chat_message("user"):
        st.write(query)

    # 2. æ‰§è¡Œæ£€ç´¢
    with st.chat_message("assistant"):
        st.write("ğŸ” **Retrieving context from Knowledge Graph...**")

        # è°ƒç”¨ HippoRAG æ£€ç´¢ï¼ˆç”¨äºå¯è§†åŒ–ä¸Šä¸‹æ–‡ï¼‰
        retrieved_contents, retrieved_ids = retriever.retrieve(query, topN=top_k)

        if not retrieved_ids:
            st.warning("No relevant information found.")
        else:
            # --- 3. å±•ç¤ºæ£€ç´¢ç»“æœ (å¯è§†åŒ–æ ¸å¿ƒ) ---
            tabs = st.tabs([f"Chunk {i+1}" for i in range(len(retrieved_ids))])

            for i, tab in enumerate(tabs):
                content = retrieved_contents[i]
                chunk_id = retrieved_ids[i]

                with tab:
                    st.caption(f"Source ID: `{chunk_id}`")

                    # è§£ææ–‡æœ¬ä¸­çš„å›¾ç‰‡æ ‡ç­¾å¹¶æ¸²æŸ“
                    parts = re.split(r'(\(Image: IMG_[^\)]+\))', content)

                    for part in parts:
                        img_match = re.match(r'\(Image: (IMG_[^\)]+)\)', part)
                        if img_match:
                            img_id = img_match.group(1)
                            img_url = image_map.get(img_id)

                            if img_url:
                                st.image(img_url, caption=f"{img_id}", width=400)
                            else:
                                st.warning(f"âš ï¸ Image found in text [{img_id}] but URL missing in map.")
                        else:
                            if part.strip():
                                st.markdown(part)

            # --- 4. ä½¿ç”¨å¤šæ¨¡æ€ ReAct ç”Ÿæˆå›ç­” ---
            st.divider()
            st.markdown("### ğŸ¤– AI Answer (Multimodal ReAct)")

            with st.spinner("Running multimodal ReAct reasoning..."):
                mm_react = MultimodalReAct(llm_gen)
                answer, history = mm_react.generate_with_rag_react(
                    question=query,
                    retriever=retriever,
                    image_map=image_map,
                    max_iterations=5,
                    max_new_tokens=1024,
                    logger=None,
                )

                st.write(answer)

                # å±•ç¤º ReAct æ¨ç†è¿‡ç¨‹ï¼ˆå¯é€‰ï¼‰
                if history:
                    with st.expander("Show ReAct search history"):
                        for i, (thought, action, observation) in enumerate(history):
                            st.markdown(f"**Step {i+1}**")
                            st.markdown(f"- **Thought**: {thought}")
                            st.markdown(f"- **Action**: {action}")
                            st.markdown(f"- **Observation**: {observation}")
                            st.markdown("---")